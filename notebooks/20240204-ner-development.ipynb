{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition Model\n",
    "In this python notebook, a NER model will be created by using the data scraped from LinkedIn jobs.\n",
    "The steps are:\n",
    "1. Data Cleaning\n",
    "2. Data Annotating\n",
    "3. Data Preparation\n",
    "4. Data Modelling\n",
    "5. Evaluation\n",
    "6. Predict on Test Data\n",
    "7. Visualize data using Tableau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    @staticmethod\n",
    "    def load_data(path):\n",
    "        return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self, model_name: str = \"en_core_web_sm\"):\n",
    "        # Load spaCy \n",
    "        self.nlp = spacy.load(model_name)\n",
    "        disabled_pipes = [pipe for pipe in self.nlp.pipe_names if pipe not in ['tokenizer', 'tagger', 'attribute_ruler']]\n",
    "        for pipe in disabled_pipes:\n",
    "            self.nlp.disable_pipe(pipe)\n",
    "\n",
    "    def preprocess_job_desc(self, text):\n",
    "        # Text Preprocessing using Spacy\n",
    "        text = re.sub(\"<[^>]+>\", \" \", text)  # remove html element tags\n",
    "        text = re.sub(\"[ ]+\", \" \", text)  # remove long spaces\n",
    "        text = re.sub(\"[^\\u0000-\\u007F]+\", \"\", text)  # remove unicode characters/ non ASCII characters\n",
    "        text = text.lower()  # transform to lower case\n",
    "        text = text.strip()\n",
    "        doc = self.nlp(text)\n",
    "        return \" \".join([word.text for word in doc])\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_to_doc_bin(data):\n",
    "        blank_nlp = spacy.blank('en')\n",
    "        db = DocBin()\n",
    "        for text, annotations in data:\n",
    "            doc = blank_nlp(text)\n",
    "            ents = []\n",
    "            for start, end, label in annotations['entities']:\n",
    "                span = doc.char_span(start, end, label=label)\n",
    "                if type(span) is not type(None):\n",
    "                    ents.append(span)\n",
    "            doc.ents = ents\n",
    "            db.add(doc)\n",
    "        return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = DataLoader.load_data(\"../data/raw/linkedin_jobs_train.csv\")\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Preprocess Job Descriptions\n",
    "texts = []\n",
    "data_preprocessor = DataPreprocessor(model_name=\"en_core_web_sm\")\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    texts.append(data_preprocessor.preprocess_job_desc(row['job_description'])+\"\\n\")\n",
    "\n",
    "# Save Jobs in form of jobs.txt\n",
    "with open(\"../data/interim/jobs_train.txt\", \"w\") as f:\n",
    "    f.writelines(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Annotating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotate data using this link https://tecoholic.github.io/ner-annotator/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation \n",
    "\n",
    "Split data for train set, development set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Train and Dev\n",
    "# Open the annotations\n",
    "with open(\"../data/interim/annotations.json\", \"r\") as f:\n",
    "    annotations_json = json.load(f)\n",
    "\n",
    "# Set params for random select data\n",
    "total_size = len(annotations_json['annotations'])\n",
    "train_len, dev_len, test_len = int(total_size * 0.7), int(total_size * 0.15), int(total_size * 0.15) \n",
    "data = []\n",
    "for aj in annotations_json['annotations']:\n",
    "    data.append((aj[0], aj[1]))    \n",
    "\n",
    "# Randomized the data\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Prepare Train Data\n",
    "train_data = data[:train_len] \n",
    "db = DataPreprocessor.convert_to_doc_bin(train_data)\n",
    "db.to_disk(\"../data/processed/train.spacy\")\n",
    "\n",
    "# Prepare Dev Data\n",
    "dev_data = data[train_len : train_len + dev_len] \n",
    "db = DataPreprocessor.convert_to_doc_bin(dev_data)\n",
    "db.to_disk(\"../data/processed/dev.spacy\")\n",
    "\n",
    "# Prepare Test Data\n",
    "test_data = data[train_len + dev_len:] \n",
    "for data in test_data:\n",
    "    texts.append(data[0])\n",
    "\n",
    "with open(\"../data/interim/jobs_test.txt\", \"w\") as f:\n",
    "    f.writelines(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Modelling (From Scratch)\n",
    "Steps\n",
    "1. Download base configuration from spacy website according to NER modelling needs, create configuration for our training.\n",
    "2. Train the model using the configuration along with the train data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "!python -m spacy init fill-config ../config/base_config.cfg ../config/config.cfg\n",
    "# 2\n",
    "!python -m spacy train ../config/config.cfg --output ../models --paths.train ../data/processed/train.spacy --paths.dev ../data/processed/dev.spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation\n",
    "Pick the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../reports/figures/evaluation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Predict on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Test Data\n",
    "with open(\"../data/interim/jobs_test.txt\", \"r\") as f:\n",
    "    test_data = f.readlines()\n",
    "\n",
    "# Access the Test Data and load Spacy Model\n",
    "best_model = spacy.load(\"../models/model-best\")\n",
    "i = 0\n",
    "hard_skills = []\n",
    "soft_skills = []\n",
    "for text in test_data:\n",
    "    docs = best_model(text)\n",
    "    for doc in docs.ents:\n",
    "        text = doc.text\n",
    "        text = re.sub(\" - \", \"-\", text)\n",
    "        text = re.sub(\" / \", \"/\", text)\n",
    "        text = re.sub(\" \", \"_\", text)\n",
    "        if doc.label_ == \"HARD SKILL\":\n",
    "            hard_skills.append(text)\n",
    "        elif doc.label_ == \"SOFT SKILL\":\n",
    "            text = re.sub(\"-\", \"_\", text)\n",
    "            soft_skills.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the list and lowercase all the words\n",
    "text = ' '.join(hard_skills).lower()\n",
    "\n",
    "#create the wordcloud object\n",
    "wordcloud = WordCloud().generate(text)\n",
    "\n",
    "#plot the wordcloud object\n",
    "plt.imshow(wordcloud, interpolation='bilInear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_skill_df = pd.DataFrame({\n",
    "    \"hard_skill\": hard_skills,\n",
    "})\n",
    "hard_skill_df['hard_skill'].value_counts().sort_values(ascending=True)[-30:].plot(kind=\"barh\", figsize=(10, 7), title=\"Top 30 Data Scientist Hard Skills\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soft Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the list and lowercase all the words\n",
    "text = ' '.join(soft_skills).lower()\n",
    "\n",
    "#create the wordcloud object\n",
    "wordcloud = WordCloud().generate(text)\n",
    "\n",
    "#plot the wordcloud object\n",
    "plt.imshow(wordcloud, interpolation='bilInear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_skill_df = pd.DataFrame({\n",
    "    \"soft_skill\": soft_skills,\n",
    "})\n",
    "soft_skill_df['soft_skill'].value_counts().sort_values(ascending=True)[-30:].plot(kind=\"barh\", figsize=(10, 7), title=\"Top 30 Data Scientist Soft Skills\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "hard_skill_df.to_csv(\"../reports/hard_skill.csv\", index=False)\n",
    "soft_skill_df.to_csv(\"../reports/soft_skill.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize in Tableau\n",
    "Link: https://public.tableau.com/views/DataScienceJobSkillsPlatform/Frontpage?:language=en-US&:display_count=n&:origin=viz_share_link"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
