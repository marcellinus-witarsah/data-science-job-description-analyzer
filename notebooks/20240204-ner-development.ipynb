{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition Model\n",
    "In this python notebook, a NER model will be created by using the data scraped from LinkedIn jobs.\n",
    "The steps are:\n",
    "1. Data Cleaning\n",
    "2. Data Annotating\n",
    "3. Data Preparation\n",
    "4. Data Modelling\n",
    "5. Evaluation\n",
    "6. Predict on Test Data\n",
    "7. Visualize data using Tableau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "sys.path.append(os.getenv(\"PROJECT_DIR\"))\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from tqdm import tqdm\n",
    "from src.data import DataLoader\n",
    "from src.data import DataPreprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "214it [00:03, 67.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = DataLoader.load_data(\"../data/raw/linkedin_jobs_train.csv\")\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Preprocess Job Descriptions\n",
    "texts = []\n",
    "data_preprocessor = DataPreprocessor(model_name=\"en_core_web_sm\")\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    texts.append(data_preprocessor.preprocess_job_desc(row['job_description'])+\"\\n\")\n",
    "\n",
    "# Save Jobs in form of jobs.txt\n",
    "with open(\"../data/interim/jobs_train.txt\", \"w\") as f:\n",
    "    f.writelines(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Annotating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotate data using this link https://tecoholic.github.io/ner-annotator/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation \n",
    "\n",
    "Split data for train set, development set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Train and Dev\n",
    "# Open the annotations\n",
    "with open(\"../data/interim/annotations.json\", \"r\") as f:\n",
    "    annotations_json = json.load(f)\n",
    "\n",
    "# Set params for random select data\n",
    "total_size = len(annotations_json['annotations'])\n",
    "train_len, dev_len, test_len = int(total_size * 0.7), int(total_size * 0.15), int(total_size * 0.15) \n",
    "data = []\n",
    "for aj in annotations_json['annotations']:\n",
    "    data.append((aj[0], aj[1]))    \n",
    "\n",
    "# Randomized the data\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Prepare Train Data\n",
    "train_data = data[:train_len] \n",
    "db = DataPreprocessor.convert_to_doc_bin(train_data)\n",
    "db.to_disk(\"../data/processed/train.spacy\")\n",
    "\n",
    "# Prepare Dev Data\n",
    "dev_data = data[train_len : train_len + dev_len] \n",
    "db = DataPreprocessor.convert_to_doc_bin(dev_data)\n",
    "db.to_disk(\"../data/processed/dev.spacy\")\n",
    "\n",
    "# Prepare Test Data\n",
    "test_data = data[train_len + dev_len:] \n",
    "db = DataPreprocessor.convert_to_doc_bin(test_data)\n",
    "db.to_disk(\"../data/processed/test.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Modelling (From Scratch)\n",
    "Steps\n",
    "1. Download base configuration from spacy website according to NER modelling needs, create configuration for our training.\n",
    "2. Train the model using the configuration along with the train data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "!python -m spacy init fill-config ../config/base_config.cfg ../config/config.cfg\n",
    "# 2\n",
    "!python -m spacy train ../config/config.cfg --output ../models --paths.train ../data/processed/train.spacy --paths.dev ../data/processed/dev.spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation\n",
    "Pick the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../reports/figures/evaluation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Predict on Test Data & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "from spacy.training import Example\n",
    "from spacy.scorer import Scorer\n",
    "test_data = DocBin().from_disk(\"../data/processed/test.spacy\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Access the Test Data and load Spacy Model\n",
    "best_model = spacy.load(\"../models/model-best\")\n",
    "i = 0\n",
    "hard_skills = []\n",
    "soft_skills = []\n",
    "\n",
    "# Create an array of Example objects between the predicted and the reference\n",
    "examples = []\n",
    "for test_doc in test_data.get_docs(nlp.vocab):\n",
    "    pred_doc = best_model(test_doc.text)\n",
    "    examples.append(Example(pred_doc, test_doc))  # for evaluation purposes\n",
    "    for doc in pred_doc.ents:\n",
    "        text = doc.text\n",
    "        text = re.sub(\" - \", \"-\", text)\n",
    "        text = re.sub(\" / \", \"/\", text)\n",
    "        text = re.sub(\" \", \"_\", text)\n",
    "        if doc.label_ == \"HARD SKILL\":\n",
    "            hard_skills.append(text)\n",
    "        elif doc.label_ == \"SOFT SKILL\":\n",
    "            text = re.sub(\"-\", \"_\", text)\n",
    "            soft_skills.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision: 0.71\n",
      "Recall: 0.68\n",
      "F1-Score: 0.70\n",
      "\n",
      "\n",
      "Precision for DEGREE entity: 0.83\n",
      "Recall for DEGREE: 0.84\n",
      "F1-Score for DEGREE: 0.83\n",
      "    \n",
      "\n",
      "Precision for SOFT SKILL entity: 0.66\n",
      "Recall for SOFT SKILL: 0.59\n",
      "F1-Score for SOFT SKILL: 0.63\n",
      "    \n",
      "\n",
      "Precision for HARD SKILL entity: 0.71\n",
      "Recall for HARD SKILL: 0.68\n",
      "F1-Score for HARD SKILL: 0.69\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "scores = Scorer.score_spans(examples, 'ents')\n",
    "print(f\"\"\"\n",
    "Precision: {scores['ents_p']:.2f}\n",
    "Recall: {scores['ents_r']:.2f}\n",
    "F1-Score: {scores['ents_f']:.2f}\n",
    "\"\"\")\n",
    "\n",
    "for k, v in scores['ents_per_type'].items():\n",
    "    print(f\"\"\"\n",
    "Precision for {k} entity: {v['p']:.2f}\n",
    "Recall for {k}: {v['r']:.2f}\n",
    "F1-Score for {k}: {v['f']:.2f}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "hard_skill_df = pd.DataFrame({\"hard_skill\": hard_skills})\n",
    "soft_skill_df = pd.DataFrame({\"soft_skill\": soft_skills})\n",
    "hard_skill_df.to_csv(\"../reports/hard_skill.csv\", index=False)\n",
    "soft_skill_df.to_csv(\"../reports/soft_skill.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize in Tableau\n",
    "Link: https://public.tableau.com/views/DataScienceJobSkillsPlatform/Frontpage?:language=en-US&:display_count=n&:origin=viz_share_link"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
