{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition Model\n",
    "In this python notebook, a NER model will be created by using the data scraped from LinkedIn jobs.\n",
    "The steps are:\n",
    "1. Data Cleaning\n",
    "2. Data Annotating\n",
    "3. Data Preparation\n",
    "4. Data Modelling\n",
    "5. Evaluation\n",
    "6. Predict on Test Data\n",
    "7. Visualize data using Tableau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "sys.path.append(os.getenv(\"PROJECT_DIR\"))\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from tqdm import tqdm\n",
    "from src.data import DataLoader\n",
    "from src.data import DataPreprocessor\n",
    "from src.models import RuleBasedNER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "214it [00:03, 67.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = DataLoader.load_data(\"../data/raw/linkedin_jobs_train.csv\")\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Preprocess Job Descriptions\n",
    "texts = []\n",
    "data_preprocessor = DataPreprocessor(model_name=\"en_core_web_sm\")\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    texts.append(data_preprocessor.preprocess_job_desc(row['job_description'])+\"\\n\")\n",
    "\n",
    "# Save Jobs in form of jobs.txt\n",
    "with open(\"../data/interim/jobs_train.txt\", \"w\") as f:\n",
    "    f.writelines(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Annotating\n",
    "To reduce effort we can perform rule based entity recognition. The idea is to annotate skills that are easily to identified, such as `python`, `machine learning`, `r`, `mysql`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run this code just once\n",
    "# # Create rule based entity recognition\n",
    "# rule_based_ner = RuleBasedNER()\n",
    "\n",
    "# # Access the hard skills and soft skills\n",
    "# with open(\"../data/external/hardskills_dictionary.txt\", 'r') as f:\n",
    "#     hard_skills = f.readlines()\n",
    "# with open(\"../data/external/softskills_dictionary.txt\", 'r') as f:\n",
    "#     soft_skills = f.readlines()\n",
    "    \n",
    "# # Add entity rule\n",
    "# rule_based_ner.add_rule(hard_skills, 'HARD SKILL')\n",
    "# rule_based_ner.add_rule(soft_skills, 'SOFT SKILL')\n",
    "\n",
    "# # Save the rule for reused\n",
    "# rule_based_ner.to_disk(\"../data/interim/patterns.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [00:03<00:00, 69.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load entity \n",
    "nlp = RuleBasedNER(pattern_path=\"../data/interim/patterns.jsonl\").nlp\n",
    "\n",
    "# Perform Entity Rule Based tagging\n",
    "with open(\"../data/interim/jobs_train.txt\", \"r\") as f:\n",
    "    jobs = f.readlines()\n",
    "\n",
    "pre_annotations_dict = {\n",
    "    \"classes\": [\"HARD SKILL\", \"SOFT SKILL\"],\n",
    "    \"annotations\": []\n",
    "}\n",
    "\n",
    "for job in tqdm(jobs):\n",
    "    text = job.strip()+\"\\r\"  # add the /r\n",
    "    doc = nlp(text)\n",
    "    ents_dict = {\"entities\": [[ent.start_char, ent.end_char, ent.label_] for ent in doc.ents]}\n",
    "    pre_annotations_dict['annotations'].append([text, ents_dict])\n",
    "\n",
    "# Save annotations\n",
    "with open(\"../data/interim/pre_annotations.json\" , 'w') as f:\n",
    "    json.dump(pre_annotations_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotate the rest of the unlabeled entities by using this tool https://tecoholic.github.io/ner-annotator/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation \n",
    "\n",
    "Split data for train set, development set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Train and Dev\n",
    "# Open the annotations\n",
    "with open(\"../data/interim/annotations.json\", \"r\") as f:\n",
    "    annotations_json = json.load(f)\n",
    "\n",
    "# Set params for random select data\n",
    "total_size = len(annotations_json['annotations'])\n",
    "train_len, dev_len, test_len = int(total_size * 0.7), int(total_size * 0.15), int(total_size * 0.15) \n",
    "data = []\n",
    "for aj in annotations_json['annotations']:\n",
    "    data.append((aj[0], aj[1]))    \n",
    "\n",
    "# Randomized the data\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Prepare Train Data\n",
    "train_data = data[:train_len] \n",
    "db = DataPreprocessor.convert_to_doc_bin(train_data)\n",
    "db.to_disk(\"../data/processed/train.spacy\")\n",
    "\n",
    "# Prepare Dev Data\n",
    "dev_data = data[train_len : train_len + dev_len] \n",
    "db = DataPreprocessor.convert_to_doc_bin(dev_data)\n",
    "db.to_disk(\"../data/processed/dev.spacy\")\n",
    "\n",
    "# Prepare Test Data\n",
    "test_data = data[train_len + dev_len:] \n",
    "db = DataPreprocessor.convert_to_doc_bin(test_data)\n",
    "db.to_disk(\"../data/processed/test.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Modelling (From Scratch)\n",
    "Steps\n",
    "1. Download base configuration from spacy website according to NER modelling needs, create configuration for our training.\n",
    "2. Train the model using the configuration along with the train data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "!python -m spacy init fill-config ../config/base_config.cfg ../config/config.cfg\n",
    "# 2\n",
    "!python -m spacy train ../config/config.cfg --output ../models --paths.train ../data/processed/train.spacy --paths.dev ../data/processed/dev.spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation\n",
    "Pick the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../reports/figures/evaluation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Predict on Test Data & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "from spacy.training import Example\n",
    "from spacy.scorer import Scorer\n",
    "test_data = DocBin().from_disk(\"../data/processed/test.spacy\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Access the Test Data and load Spacy Model\n",
    "best_model = spacy.load(\"../models/model-best\")\n",
    "i = 0\n",
    "hard_skills = []\n",
    "soft_skills = []\n",
    "\n",
    "# Create an array of Example objects between the predicted and the reference\n",
    "examples = []\n",
    "for test_doc in test_data.get_docs(nlp.vocab):\n",
    "    pred_doc = best_model(test_doc.text)\n",
    "    examples.append(Example(pred_doc, test_doc))  # for evaluation purposes\n",
    "    for doc in pred_doc.ents:\n",
    "        text = doc.text\n",
    "        text = re.sub(\" - \", \"-\", text)\n",
    "        text = re.sub(\" / \", \"/\", text)\n",
    "        text = re.sub(\" \", \"_\", text)\n",
    "        if doc.label_ == \"HARD SKILL\":\n",
    "            hard_skills.append(text)\n",
    "        elif doc.label_ == \"SOFT SKILL\":\n",
    "            text = re.sub(\"-\", \"_\", text)\n",
    "            soft_skills.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision: 0.71\n",
      "Recall: 0.68\n",
      "F1-Score: 0.70\n",
      "\n",
      "\n",
      "Precision for DEGREE entity: 0.83\n",
      "Recall for DEGREE: 0.84\n",
      "F1-Score for DEGREE: 0.83\n",
      "    \n",
      "\n",
      "Precision for SOFT SKILL entity: 0.66\n",
      "Recall for SOFT SKILL: 0.59\n",
      "F1-Score for SOFT SKILL: 0.63\n",
      "    \n",
      "\n",
      "Precision for HARD SKILL entity: 0.71\n",
      "Recall for HARD SKILL: 0.68\n",
      "F1-Score for HARD SKILL: 0.69\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "scores = Scorer.score_spans(examples, 'ents')\n",
    "print(f\"\"\"\n",
    "Precision: {scores['ents_p']:.2f}\n",
    "Recall: {scores['ents_r']:.2f}\n",
    "F1-Score: {scores['ents_f']:.2f}\n",
    "\"\"\")\n",
    "\n",
    "for k, v in scores['ents_per_type'].items():\n",
    "    print(f\"\"\"\n",
    "Precision for {k} entity: {v['p']:.2f}\n",
    "Recall for {k}: {v['r']:.2f}\n",
    "F1-Score for {k}: {v['f']:.2f}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "hard_skill_df = pd.DataFrame({\"hard_skill\": hard_skills})\n",
    "soft_skill_df = pd.DataFrame({\"soft_skill\": soft_skills})\n",
    "hard_skill_df.to_csv(\"../reports/hard_skill.csv\", index=False)\n",
    "soft_skill_df.to_csv(\"../reports/soft_skill.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize in Tableau\n",
    "Link: https://public.tableau.com/views/DataScienceJobSkillsPlatform/Frontpage?:language=en-US&:display_count=n&:origin=viz_share_link"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
